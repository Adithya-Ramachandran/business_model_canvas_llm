# gemini_client.py

import streamlit as st
import google.generativeai as genai
from canvas_state import update_canvas_data_from_session

# --- Configure and Initialize Gemini API ---
try:
    gemini_api_key = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=gemini_api_key)
    gemini_model = genai.GenerativeModel('gemini-2.5-pro')
except (KeyError, Exception) as e:
    st.error(f"Failed to configure or initialize Gemini model: {e}")
    st.stop()


def get_gemini_response(prompt_text):
    """Makes an API call to the Gemini model."""
    try:
        response = gemini_model.generate_content(prompt_text)
        if response.candidates:
            return response.candidates[0].content.parts[0].text
        else:
            return "No text content generated by LLM."
    except Exception as e:
        st.error(f"Error calling Gemini API: {e}")
        return "An error occurred while fetching a response from Gemini."


def get_canvas_summary():
    """Compiles a summary of all filled canvas sections."""
    summary_parts = []
    update_canvas_data_from_session()
    for key, value in st.session_state.canvas_data.items():
        if value and key != "business_idea_description":
            readable_key = key.replace("_", " ").title()
            summary_parts.append(f"- {readable_key}: {value}")

    if not summary_parts:
        return "No other canvas sections have been filled yet."
    return "Current overview of other canvas sections:\n" + "\n".join(summary_parts)


def handle_llm_request(user_prompt, target_key, section_title, mode):
    """
    Generates an LLM response based on a specific interaction mode
    and adds it to the chat history.
    """
    st.session_state.llm_chat_history.append({"role": "user", "content": user_prompt, "target_key": target_key})

    business_idea = st.session_state.get("business_idea_description",
                                         "").strip() or "The user has not provided an overall business idea description yet."
    canvas_summary = get_canvas_summary()
    current_content = st.session_state.get(target_key, "N/A")

    # --- NEW: Mode-specific instructions for the LLM ---
    mode_instructions = {
        "Ask Critical Questions": """
        Your task is to act as a Socratic guide. Do NOT give direct answers or suggestions. 
        Instead, ask 2-3 probing, open-ended questions that force the user to think more deeply about their entry. 
        Focus on 'Why', 'What if', 'How', and 'Can you be more specific?'. Your goal is to help the user validate their own assumptions.
        """,
        "Play Devil's Advocate": """
        Your task is to constructively challenge the user's assumptions for this section. 
        Politely point out potential flaws, risks, or alternative viewpoints. 
        Start your response with phrases like, 'Have you considered...?', 'A potential risk with that approach might be...', or 'What if the market reacts differently? For example...'.
        """,
        "Break It Down": """
        Your task is to help the user deconstruct their ideas. Take the user's input for this section and help them break it down into smaller, more concrete components. 
        If they mention a broad group, help them segment it. If they list a complex service, help them itemize its key features. 
        Provide a structured breakdown, perhaps using bullet points.
        """,
        "Suggest Ideas": """
        Your goal is to provide concise, actionable suggestions for this section, ideally as bullet points or a short paragraph. 
        Use the context provided to generate creative and relevant ideas.
        """
    }

    instructional_prompt = mode_instructions.get(mode, mode_instructions["Suggest Ideas"])

    full_prompt = f"""
    You are an expert business strategist acting as a thinking partner for a user filling out a Business Model Canvas.

    **Interaction Mode:** {mode}
    **Your instructions for this mode:** {instructional_prompt}

    **Context:**
    - **Overall Business Idea:** {business_idea}
    - **Overview of other canvas sections:**
    {canvas_summary}

    **User's Focus:**
    - **Canvas Section:** '{section_title}'
    - **Current Content in this Section:** '{current_content}'
    - **User's Specific Request:** '{user_prompt}'

    Based on your instructions for the selected interaction mode and all the context provided, please provide your response.
    """

    with st.spinner(f"Gemini thinking in '{mode}' mode..."):
        llm_response = get_gemini_response(full_prompt)

    st.session_state.llm_chat_history.append({"role": "llm", "content": llm_response, "target_key": target_key})
    st.rerun()